{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03568e-1b3e-4a11-a6fc-8e5587dc3e74",
   "metadata": {
    "id": "cb03568e-1b3e-4a11-a6fc-8e5587dc3e74"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import argparse\n",
    "from datetime import datetime,date\n",
    "from io import StringIO\n",
    "import csv\n",
    "import country_converter as coco\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FPE3CpuMcA7b",
   "metadata": {
    "id": "FPE3CpuMcA7b"
   },
   "outputs": [],
   "source": [
    "# Please put your OpenAI API key here in place of the red string\n",
    "client = OpenAI(api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56255100-b010-4bba-b5e3-8a3a711fde70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "56255100-b010-4bba-b5e3-8a3a711fde70",
    "outputId": "b026e1b2-ada4-4ea0-dd10-5ce01aef39bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload the files here.\n",
      "Please do not make any edits to the file(s) you downloaded from Sourcescrub.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-98366d36-27ab-4681-9cc0-dd524a5e592f\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-98366d36-27ab-4681-9cc0-dd524a5e592f\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Sourcescrub - Random List 2025.08.24 190558252.csv to Sourcescrub - Random List 2025.08.24 190558252 (10).csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Upload the files here.\\nPlease do not make any edits to the file(s) you downloaded from Sourcescrub.\")\n",
    "uploaded = files.upload()  # opens a file picker in the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C3zJ5yHnuDOf",
   "metadata": {
    "id": "C3zJ5yHnuDOf"
   },
   "outputs": [],
   "source": [
    "def find_header_row(inputpath):\n",
    "  with open(inputpath, \"r\") as f:\n",
    "      for i in range(10):\n",
    "          row=f.readline()\n",
    "          if(row.startswith(\"Company Name,\")):\n",
    "            # print(i)\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673dd117-28c4-49b8-aff6-af7f768dbcd8",
   "metadata": {
    "id": "673dd117-28c4-49b8-aff6-af7f768dbcd8"
   },
   "outputs": [],
   "source": [
    "def scrape_website(url, max_pages=5, delay=1, verbose=False):\n",
    "    \"\"\"\n",
    "    Scrape all text content from a website.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        max_pages (int, optional): Maximum number of pages to scrape. Defaults to 10.\n",
    "        delay (int, optional): Delay between requests in seconds. Defaults to 1.\n",
    "        verbose (bool, optional): Whether to print progress information. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        str: All text content from the website.\n",
    "    \"\"\"\n",
    "    # Validate URL\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'https://' + url\n",
    "\n",
    "    # Initialize variables\n",
    "    visited_urls = set()\n",
    "    to_visit = [url]\n",
    "    base_domain = urlparse(url).netloc\n",
    "    all_text = []\n",
    "\n",
    "    # Set up headers to mimic a browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "    }\n",
    "\n",
    "    # Crawl the website\n",
    "    page_count = 0\n",
    "    while to_visit and page_count < max_pages:\n",
    "        current_url = to_visit.pop(0)\n",
    "\n",
    "        # Skip if already visited\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        # Add to visited set\n",
    "        visited_urls.add(current_url)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Scraping: {current_url}\")\n",
    "\n",
    "        try:\n",
    "            # Make the request\n",
    "            response = requests.get(current_url, headers=headers, timeout=10)\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if response.status_code != 200:\n",
    "                if verbose:\n",
    "                    print(f\"Failed to retrieve {current_url}: Status code {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract all text\n",
    "            page_text = extract_text_from_soup(soup)\n",
    "            all_text.append(f\"--- Content from: {current_url} ---\\n{page_text}\\n\")\n",
    "\n",
    "            # Find all links on the page\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "\n",
    "                # Skip empty links, anchors, and non-HTTP links\n",
    "                if not href or href.startswith('#') or href.startswith('javascript:') or href.startswith('mailto:'):\n",
    "                    continue\n",
    "\n",
    "                # Convert relative URLs to absolute URLs\n",
    "                absolute_url = urljoin(current_url, href)\n",
    "\n",
    "                # Only follow links to the same domain\n",
    "                if urlparse(absolute_url).netloc == base_domain and absolute_url not in visited_urls:\n",
    "                    to_visit.append(absolute_url)\n",
    "\n",
    "            # Increment page count\n",
    "            page_count += 1\n",
    "\n",
    "            # Respect robots.txt by adding a delay\n",
    "            time.sleep(delay)\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error scraping {current_url}: {str(e)}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Scraped {page_count} pages from {url}\")\n",
    "\n",
    "    # Join all text with newlines\n",
    "    return '\\n'.join(all_text)\n",
    "\n",
    "def extract_text_from_soup(soup):\n",
    "    \"\"\"\n",
    "    Extract all text from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object.\n",
    "\n",
    "    Returns:\n",
    "        str: All text content.\n",
    "    \"\"\"\n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup(['script', 'style', 'meta', 'noscript']):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    # Get text\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    # Clean up text\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace with single space\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Replace multiple newlines with double newlines\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LdV2iqVbxjML",
   "metadata": {
    "id": "LdV2iqVbxjML"
   },
   "outputs": [],
   "source": [
    "def _get_encoding(model_name=\"gpt-4o\"):\n",
    "    try:\n",
    "        import tiktoken\n",
    "        try:\n",
    "            return tiktoken.encoding_for_model(model_name)\n",
    "        except Exception:\n",
    "            # generic fallback works for GPT-4/3.5/4o families\n",
    "            return tiktoken.get_encoding(\"cl100k_base\")\n",
    "    except Exception:\n",
    "        return None  # no tiktoken installed\n",
    "\n",
    "def count_tokens(text: str, model_name=\"gpt-4o\") -> int:\n",
    "    enc = _get_encoding(model_name)\n",
    "    if enc:\n",
    "        return len(enc.encode(text))\n",
    "    # crude fallback: ~4 chars ≈ 1 token\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "def truncate_to_tokens(text: str, max_tokens: int, model_name=\"gpt-4o-mini\", reserve_tokens: int = 0) -> str:\n",
    "    \"\"\"Return text truncated to <= max_tokens, keeping a safety reserve.\"\"\"\n",
    "    effective = max(0, max_tokens - max(0, reserve_tokens))\n",
    "    if effective <= 0:\n",
    "        return \"\"\n",
    "    enc = _get_encoding(model_name)\n",
    "    if enc:\n",
    "        ids = enc.encode(text)\n",
    "        if len(ids) <= effective:\n",
    "            return text\n",
    "        return enc.decode(ids[:effective])\n",
    "    # fallback by characters if no tiktoken\n",
    "    max_chars = effective * 4\n",
    "    return text[:max_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93257690-0695-4d1f-9b79-40c05ff6cd31",
   "metadata": {
    "id": "93257690-0695-4d1f-9b79-40c05ff6cd31"
   },
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    # Use regex to find the first {...} block\n",
    "    match = re.search(r'\\{.*?\\}', text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"JSON decode error:\", e)\n",
    "    else:\n",
    "        print(\"No JSON object found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8131b55-a2e7-4679-a6ac-79d4212ee40e",
   "metadata": {
    "id": "a8131b55-a2e7-4679-a6ac-79d4212ee40e"
   },
   "outputs": [],
   "source": [
    "def generate_notes(company_name, company_description, employee_count, url):\n",
    "    \"\"\"\n",
    "    Sends conversation messages to OpenAI and optionally includes\n",
    "    attachment content in the first user prompt.\n",
    "    \"\"\"\n",
    "    # Add attachment content to the first user message if exists\n",
    "    website_data = scrape_website(url)\n",
    "    capped_text = truncate_to_tokens(website_data, 20000, model_name=\"gpt-4o\", reserve_tokens=1500)\n",
    "\n",
    "    prompt_with_attachment = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that replies only in strict JSON, with keys: 'Name', 'Cheap Labor', 'Recurring', 'Category', 'Score', 'Notes'.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"I have to score (from 0-100) and take notes on this company {company_name} with this description:\\n\n",
    "        \\\"{company_description}\\\" with an employee count of {employee_count},\\n and website data\\n\n",
    "        {capped_text}\n",
    "        \\n\n",
    "        You are to first judge two things:\\n\n",
    "\n",
    "       1. If the company primarily operates as a low-cost outsourcing provider — relying heavily on labor from countries such as India, Pakistan, Croatia, the Philippines, Vietnam, Ukraine, Romania, or other regions where Western companies often outsource due to lower wage costs\\n\n",
    "        Analyze the company’s website and identify any indicators that it outsources or offshores labor. Specifically, look for: Mentions of offshore, nearshore, global delivery centers, or shared service centers. Lists of office locations in outsourcing hubs (e.g., India, Pakistan, Philippines, Vietnam, Romania, Ukraine, Croatia). Services described as Business Process Outsourcing (BPO), Knowledge Process Outsourcing (KPO), IT outsourcing, or call center operations. Careers or job postings concentrated in lower-cost regions. Language emphasizing cost savings, scalability, or providing “affordable” or “efficient” labor solutions. Case studies or client examples that highlight labor arbitrage.\n",
    "        If so, set the ‘Cheap Labor’ field in the JSON to the boolean value ‘True’ and automatically assign a score of 0. \\n\n",
    "\n",
    "       2. The likelihood that the company follows a recurring revenue business model. To determine this, you need to analyze text data from the company’s website. Look for evidence of subscription or membership pricing (monthly, annual, or per-user), SaaS or cloud-based platforms with renewals, or service offerings provided on an ongoing basis such as managed services, retainers, or long-term contracts. Pay attention to language that references renewals, auto-renewal, continuous support, or recurring billing. Also check for mentions of Annual or Monthly Recurring Revenue in investor or press materials, customer success or account management functions tied to retention, and client examples that highlight continuous or long-term engagement. The description provided ealier may also help you figure this out.\\n\n",
    "        Using this information, assign a value of ‘Low,’ ‘Medium,’ or ‘High’ to indicate the likelihood that the company has a recurring revenue model, and record this in the ‘Recurring’ field of the JSON. If the likelihood is ‘Low,’ adjust your overall rating downward significantly, ensuring that the maximum cap is set at 50 instead of 100.\\n\n",
    "\n",
    "       Now, after taking note of these things, you are to do three things:\\n\n",
    "\n",
    "        First, assign the company to a category of your choice. Example categories include Construction and Design Software, Mass Tort Software, Cybersecurity, Healthcare, Fleet Management Software, Insurance, Managed Service Provider, and Food Tech, though you are free to create your own as appropriate. Once you have determined the category, record it in the ‘Category’ field of the JSON.\\n\n",
    "\n",
    "        Second, assign the company a score from 0 to 100, focusing primarily on the quality and sustainability of its business model, while also weighing industry attractiveness, market size, and potential for growth. Use the following standards:\\n\n",
    "\n",
    "        1. 75–100 (High Quality / Strong Candidate): The company demonstrates a clearly differentiated, defensible, and scalable business model (e.g., recurring revenue, high retention, strong unit economics, attractive margins). It operates in a large and growing market with favorable industry tailwinds, competitive advantages (technology, brand, or distribution), and evidence of execution capability. These companies are highly attractive and should be prioritized for investment consideration.\\n\n",
    "        2. 50–74 (Moderate Quality / Potential Candidate): The company has a sound but less compelling business model. Revenue may be partially recurring or dependent on cyclical demand, margins may be average, or customer retention may be moderate. The market opportunity exists but may be smaller, more competitive, or slower-growing. These companies may be worth considering depending on valuation, strategic fit, or further diligence.\\n\n",
    "        3. 25–49 (Low Quality / Weak Candidate): The company’s business model raises concerns (e.g., transactional or project-based revenue, high churn risk, reliance on low-cost labor arbitrage, commoditized offerings). Industry or market outlook may be limited, shrinking, or highly fragmented, with little competitive differentiation. These companies are unlikely to be strong candidates and typically should not move forward.\\n\n",
    "        4. 0–24 (Very Poor Quality / Exclude): The company has a fundamentally flawed or unsustainable business model (e.g., no clear path to profitability, highly concentrated revenue, declining market, no defensible moat). It operates in an unattractive or shrinking industry with severe headwinds. These companies should be removed from the sheet and excluded from consideration.\\n\n",
    "\n",
    "       The companies here should be judged with absolute rigor. Do not inflate scores—apply a cold, objective lens. In particular, avoid giving high marks to categories outside our focus: hardware manufacturers, law firms (note: legal services and legal software are acceptable), investment firms (though service providers for them may be relevant), hospitals, or traditional retail stores without a meaningful technology component. Our target profile is enterprise software and tech-enabled services businesses. These are the models that should earn the strongest scores.\\n\n",
    "       And when you are finished, put the numerical score under the 'Score' section on the json.\\n\n",
    "\n",
    "        Finally, add a note justifying the score, referencing specific aspects of the company, product, or market when relevant. Keep it under 40 words, in notes form (ideally not full sentences), and record it in the ‘Notes’ field of the JSON. Do not simply write phrases like ‘Strong recurring revenue’ or ‘Highly recurring revenue.’ Instead, explain why it is recurring, pointing to specific features of the product, customer contracts, or market dynamics.\\n\n",
    "\n",
    "        PLEASE respond in JSON format with keys: 'Name', 'Cheap Labor', 'Recurring', 'Category', 'Score', 'Notes'. I want your output to be ready to be converted to JSON \\n\\n\"\"\"}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=prompt_with_attachment\n",
    "    )\n",
    "    assistant_reply = response.choices[0].message.content\n",
    "    try:\n",
    "        result = extract_json(assistant_reply)\n",
    "        df = pd.DataFrame([result])\n",
    "    except (json.JSONDecodeError, KeyError, ValueError, AttributeError):\n",
    "        print(\"GPT Output Invalid; Attempting Prompt Again\")\n",
    "        df = generate_notes(company_name, company_description, employee_count, url)\n",
    "\n",
    "    try:\n",
    "        test_name = df.loc[0,'Name']\n",
    "        test_cheap = df.loc[0,'Cheap Labor']\n",
    "        test_recurring = df.loc[0,'Recurring']\n",
    "        test_category = df.loc[0,'Category']\n",
    "        test_score = df.loc[0,'Score']\n",
    "        test_notes = df.loc[0,'Notes']\n",
    "    except (KeyError, ValueError, AttributeError):\n",
    "        print(\"GPT Output DataFrame Invalid:\")\n",
    "        print(df)\n",
    "        print(\"Attempting Prompt Again\")\n",
    "        df = generate_notes(company_name, company_description, employee_count, url)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f02089-343f-4149-8cc3-4939c6e660eb",
   "metadata": {
    "id": "17f02089-343f-4149-8cc3-4939c6e660eb"
   },
   "outputs": [],
   "source": [
    "def find_overlap(investors_str):\n",
    "    if pd.isna(investors_str):\n",
    "        return None\n",
    "    check_list = [\n",
    "    \"ABRY Partners\", \"Accel Management\", \"Sequoia Capital\",\n",
    "    \"Coatue Management\", \"Advent International\", \"Andreessen Horowitz\",\n",
    "    \"Ares Management\", \"Audax Private Equity\", \"Bain Capital\",\n",
    "    \"Battery Ventures\", \"Berkshire Partners\", \"Blackstone Group\",\n",
    "    \"Clayton, Dubilier and Rice\", \"Clearlake Capital Group\", \"CVC Capital Partners\",\n",
    "    \"Diversis Capital\", \"EQT\", \"Francisco Partners\",\n",
    "    \"FTV Capital\", \"General Catalyst\", \"Genstar Capital\",\n",
    "    \"GI Partners\", \"Golden Gate\", \"Great Hill Partners\",\n",
    "    \"GTCR\", \"H.I.G. Capital\", \"HgCapital\",\n",
    "    \"Index Ventures\", \"Institutional Venture Partners (IVP)\", \"Insight Partners\",\n",
    "    \"JMI Equity\", \"Jerusalem Venture Partners (JVP)\", \"K1 Investment Management\",\n",
    "    \"Kohlberg Kravis Roberts & Co.\", \"Kleiner Perkins\", \"Level Equity Management\",\n",
    "    \"Lightspeed\", \"LLR Partners\", \"Main Capital Partners\",\n",
    "    \"Marlin Equity Partners\", \"New Enterprise Associates\", \"New Mountain Capital\",\n",
    "    \"Norwest Venture Partners\", \"OpenView Venture Partners\", \"Parthenon Capital Partners\",\n",
    "    \"Permira\", \"Platinum Equity\", \"Providence Equity Partners\",\n",
    "    \"Quad-C Management\", \"Riverside Partners\", \"Sageview Capital\",\n",
    "    \"Serent Capital\", \"Silver Lake\", \"Spectrum Equity\",\n",
    "    \"Stone Point Capital\", \" Summit Partners\", \"Susquehanna Growth Equity\",\n",
    "    \"TA Associates Management\", \"Tiger Global Management\",\n",
    "    \"The Carlyle Group\", \"Thoma Bravo\", \"Vista Equity Partners\",\n",
    "    \"TPG\", \"Warburg Pincus\", \"Volition Capital\",\n",
    "    \"Welsh, Carson, Anderson and Stowe Management\", \"PSG Equity\"\n",
    "]\n",
    "    overlap = \"\"\n",
    "    for company in check_list:\n",
    "      if investors_str.find(company) != -1:\n",
    "        if overlap != \"\":\n",
    "          overlap = overlap + \", \" + company\n",
    "        else:\n",
    "          overlap = company\n",
    "\n",
    "    if overlap == \"\":\n",
    "      return None\n",
    "    else:\n",
    "      return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xxxDbaGrlL0U",
   "metadata": {
    "id": "xxxDbaGrlL0U"
   },
   "outputs": [],
   "source": [
    "#investors_str = \"Sequoia Capital Operations LLC, Coatue Management\"\n",
    "#investors = {inv.strip() for inv in investors_str.split(\",\")}\n",
    "#print(investors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8bcc2-7c96-4f99-bafe-915e2fdbbc08",
   "metadata": {
    "id": "78f8bcc2-7c96-4f99-bafe-915e2fdbbc08"
   },
   "outputs": [],
   "source": [
    "def is_bad_country(country):\n",
    "    if(pd.notna(country)):\n",
    "      cc = coco.CountryConverter()\n",
    "      continent = cc.convert(names=country, to='continent')\n",
    "      region = cc.convert(names=country, to='UNregion')\n",
    "      if(country == \"IL\" or region == \"Northern America\" or continent == \"Europe\" or continent == \"Oceania\"):\n",
    "          return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15cdcd-e9b3-4935-a10f-a136b294f190",
   "metadata": {
    "id": "ac15cdcd-e9b3-4935-a10f-a136b294f190"
   },
   "outputs": [],
   "source": [
    "floor = 0\n",
    "ceiling = 100\n",
    "\n",
    "def eval_growth(growth):\n",
    "    if pd.isna(growth):\n",
    "      return -1\n",
    "    return max(floor, min(ceiling, 25 + 5 * growth))\n",
    "\n",
    "def eval_raised(total_raised):\n",
    "    if pd.isna(total_raised):\n",
    "      return -1\n",
    "    raised_in_mil = float(total_raised/1_000_000)\n",
    "    if(raised_in_mil <= 5):\n",
    "        return min(ceiling, 100 - 5 * raised_in_mil)\n",
    "    else:\n",
    "        return max(floor, 75 - 25 * (raised_in_mil - 5) / 15)\n",
    "\n",
    "def eval_date(date_of_investment):\n",
    "    if pd.isna(date_of_investment):\n",
    "        return -1\n",
    "    try:\n",
    "        delta_date = (date.today() - date_of_investment.date()).days / 365.25\n",
    "    except (TypeError, AttributeError, ValueError):\n",
    "        return -2\n",
    "    if(delta_date >= 6):\n",
    "        return max(floor, min(ceiling, 5 * delta_date + 45))\n",
    "    elif(delta_date <=5):\n",
    "        return max(floor, min(ceiling, 15 * delta_date - 10))\n",
    "    else:\n",
    "        return max(floor, min(ceiling, 10 * delta_date + 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ae2d3-9212-40c7-856e-19de401556ab",
   "metadata": {
    "id": "3a5ae2d3-9212-40c7-856e-19de401556ab"
   },
   "outputs": [],
   "source": [
    "def prefilter(output_df,ownership, investors, country,employee_count):\n",
    "    if(ownership == \"Public\"):\n",
    "        output_df.loc[0,'GPT Rationale'] = f\"Tier 4: Public Company\"\n",
    "    elif(ownership == \"Private Sub\"):\n",
    "        output_df.loc[0,'GPT Rationale'] = f\"Tier 4: Private Subsidiary\"\n",
    "    elif(ownership == \"Public Sub\"):\n",
    "        output_df.loc[0,'GPT Rationale'] = f\"Tier 4: Public Subsidiary\"\n",
    "    elif(find_overlap(investors)):\n",
    "        overlap = find_overlap(investors)\n",
    "        output_df.loc[0,'GPT Rationale'] = f\"Tier 4: {overlap} on the Cap Table\"\n",
    "    elif(is_bad_country(country)):\n",
    "        output_df.loc[0,'GPT Rationale'] = f\"Tier 4: Outside Our Preferred Geographies\"\n",
    "    elif(employee_count > 1000):\n",
    "        output_df.loc[0,'GPT Rationale'] = f\"Tier 4: Over 1000 Employees\"\n",
    "    elif(employee_count < 30):\n",
    "        output_df.loc[0,'GPT Rationale'] = f\"Tier 4: Under 30 Employees\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf6844-a05f-4c6c-a805-e6bec96f98ad",
   "metadata": {
    "id": "63cf6844-a05f-4c6c-a805-e6bec96f98ad"
   },
   "outputs": [],
   "source": [
    "def evaluate_companies(inputpath,outputpath):\n",
    "    if inputpath.lower().endswith(\".csv\"):\n",
    "        # company_info = pd.read_csv(inputpath, skiprows=find_header_row(inputpath), parse_dates=[\"Date of Most Recent Investment\"], date_format=\"%m/%d/%y\")\n",
    "        company_info = pd.read_csv(inputpath, skiprows=find_header_row(inputpath))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            if company_info[\"Date of Most Recent Investment\"].dtype == \"O\":\n",
    "                company_info[\"Date of Most Recent Investment\"] = pd.to_datetime(\n",
    "                    company_info[\"Date of Most Recent Investment\"],\n",
    "                    errors=\"coerce\",\n",
    "                )\n",
    "    elif inputpath.lower().endswith(\".xlsx\"):\n",
    "        company_info = pd.read_excel(inputpath, skiprows=find_header_row(inputpath), engine=\"openpyxl\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please upload a .csv or .xlsx file.\")\n",
    "\n",
    "\n",
    "    init = {\n",
    "        \"Name\": [],\n",
    "        \"Tier\": [],\n",
    "        \"Score\": [],\n",
    "        \"GPT Rationale\": [],\n",
    "        \"Category\": [],\n",
    "        \"Recurring Revenue Likelihood\": [],\n",
    "        \"Employee Count\": [],\n",
    "        \"12 Months Growth Rate %\": [],\n",
    "        \"Total Raised\": [],\n",
    "        \"Investors\": [],\n",
    "        \"Date of Most Recent Investment\": [],\n",
    "        \"State\": [],\n",
    "        \"Country\": [],\n",
    "        \"Website\": [],\n",
    "        \"LinkedIn\": [],\n",
    "    }\n",
    "    all_output = pd.DataFrame([init])\n",
    "\n",
    "    for index,row in company_info.iterrows():\n",
    "\n",
    "        company_name = row['Company Name']\n",
    "\n",
    "        print(f\"\\n{index+1}\\n{company_name}\\n\")\n",
    "        # if index == 100:\n",
    "        #   break\n",
    "\n",
    "        ownership = row['Ownership']\n",
    "        investors = row['Investors']\n",
    "        state = row['State']\n",
    "        country = row['Country']\n",
    "\n",
    "        growth = row['12 Months Growth Rate %']\n",
    "        total_raised = row['Total Raised']\n",
    "        date_of_investment = row['Date of Most Recent Investment']\n",
    "\n",
    "        company_description = row['Description']\n",
    "        employee_count = row['Employee Count']\n",
    "        url = row['Website']\n",
    "        linkedin = row['LinkedIn Account']\n",
    "\n",
    "        output_df = pd.DataFrame([{\n",
    "                  \"Name\": company_name,\n",
    "                  \"Tier\": 4,\n",
    "                  \"Score\": 0,\n",
    "                  \"GPT Rationale\": f\"\",\n",
    "                  \"Category\": \"—\",\n",
    "                  \"Recurring Revenue Likelihood\": \"—\",\n",
    "                  \"Employee Count\": employee_count,\n",
    "                  \"12 Months Growth Rate %\": growth,\n",
    "                  \"Total Raised\": total_raised,\n",
    "                  \"Investors\": investors,\n",
    "                  \"Date of Most Recent Investment\": date_of_investment,\n",
    "                  \"State\": state,\n",
    "                  \"Country\": country,\n",
    "                  \"Website\": url,\n",
    "                  \"LinkedIn\": linkedin\n",
    "              }])\n",
    "\n",
    "\n",
    "        mini = company_info.iloc[[index]].copy()\n",
    "        cols = ['Company Name', 'Ownership', 'Investors','Country','12 Months Growth Rate %', 'Total Raised', 'Date of Most Recent Investment','Description','Employee Count','Website']\n",
    "        blank_cells = mini.reindex(columns=cols).isna().sum(axis=1).iat[0]\n",
    "\n",
    "        if blank_cells >= 8:\n",
    "            output_df.loc[0,'GPT Rationale'] = f\"Tier 4: incomplete information\"\n",
    "            print(\"Incomplete info\")\n",
    "\n",
    "        else:\n",
    "            prefiltered_df = prefilter(output_df,ownership,investors,country,employee_count)\n",
    "\n",
    "            if prefiltered_df is None:\n",
    "\n",
    "                growth_score = int(eval_growth(growth))\n",
    "                raised_score = int(eval_raised(total_raised))\n",
    "                date_score = int(eval_date(date_of_investment))\n",
    "\n",
    "                results_df = generate_notes(company_name, company_description, employee_count, url)\n",
    "                # print(\"\\nGPT Output:\")\n",
    "                # print(results_df)\n",
    "                if(results_df.loc[0,'Cheap Labor']):\n",
    "                  output_df.loc[0,'GPT Rationale'] = f\"Tier 4: Uses cheap labor from countries that offer them\"\n",
    "                  print(\"Prefiltered: \" + output_df['GPT Rationale'][0])\n",
    "\n",
    "                else:\n",
    "\n",
    "                  gpt_score = float(results_df['Score'][0])\n",
    "                  print(\"GPT Score:\" + str(gpt_score) + \"; Growth Score:\" + str(growth_score) + \"; Total Raised Score:\" + str(raised_score) + \"; Date of Recent Inv. Score:\" + str(date_score))\n",
    "\n",
    "\n",
    "                  final_score = 4 * gpt_score\n",
    "                  weights = 4\n",
    "                  if(date_score > -1):\n",
    "                    final_score += 0.5 * date_score\n",
    "                    weights += 0.5\n",
    "                  if(growth_score > -1):\n",
    "                    final_score += 1.25 * growth_score\n",
    "                    weights += 1.25\n",
    "                  if(raised_score > -1):\n",
    "                    final_score += 2 * raised_score\n",
    "                    weights += 2\n",
    "                  final_score = int(final_score / weights)\n",
    "\n",
    "                  tier = 4 - int(final_score / 25)\n",
    "\n",
    "                  output_df.loc[0,'Tier'] = np.float64(tier)\n",
    "                  output_df.loc[0,'Score'] = np.float64(final_score)\n",
    "                  output_df.loc[0,'GPT Rationale'] = results_df.loc[0,'Notes']\n",
    "                  output_df.loc[0,'Category'] = results_df.loc[0,'Category']\n",
    "                  output_df.loc[0,'Recurring Revenue Likelihood'] = results_df.loc[0,'Recurring']\n",
    "\n",
    "                  print(\"GPT Analysis incorporated:\")\n",
    "                  print(output_df.loc[0,'GPT Rationale'])\n",
    "\n",
    "            else:\n",
    "                output_df.loc[0] = prefiltered_df.loc[0]\n",
    "                print(\"Prefiltered: \" + output_df['GPT Rationale'][0])\n",
    "\n",
    "        # print(output_df)\n",
    "\n",
    "        # print(output_df.iloc[0].values)\n",
    "        all_output.loc[index,:] = output_df.iloc[0]\n",
    "\n",
    "        # Append or write with headers\n",
    "        # print(\"\\nSaving to csv\\n\")\n",
    "        all_output.to_csv(outputpath, mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oWS7mh7mxTzA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWS7mh7mxTzA",
    "outputId": "7ea79821-1fac-4263-d7bc-e9523a30aad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sourcescrub - Random List 2025.08.24 190558252 (10).csv\n",
      "\n",
      "1\n",
      "Timeless Medical Systems, Inc.\n",
      "\n",
      "GPT Score:85.0; Growth Score:96; Total Raised Score:-1; Date of Recent Inv. Score:-1\n",
      "GPT Analysis incorporated:\n",
      "High recurring from software solutions in pediatric neonatology; trusted in hospitals; extensive customer retention and safety-improving technology; large market potential in neonatal nutrition management.\n",
      "\n",
      "2\n",
      "EQT AB\n",
      "\n",
      "Prefiltered: Tier 4: Over 1000 Employees\n",
      "\n",
      "3\n",
      "Assaia International AG\n",
      "\n",
      "GPT Score:85.0; Growth Score:100; Total Raised Score:-1; Date of Recent Inv. Score:15\n",
      "GPT Analysis incorporated:\n",
      "Subscription model; strategic partnerships; strong AI-based optimization; global airport clientele; operational efficiency improvement.\n",
      "\n",
      "4\n",
      "Harvey,\n",
      "\n",
      "Prefiltered: Tier 4: Sequoia Capital, Coatue Management, Kleiner Perkins on the Cap Table\n",
      "✅ Saved all tiered sheets into /content/Companies_Tiered_All.xlsx\n"
     ]
    }
   ],
   "source": [
    "for filename in uploaded.keys():\n",
    "    print(f\"\\n{filename}\")\n",
    "    output_filename = f\"/content/Tiered_{filename}\"\n",
    "    evaluate_companies(filename,output_filename)\n",
    "\n",
    "output_excel = \"/content/Companies_Tiered_All.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_excel, engine=\"xlsxwriter\") as writer:\n",
    "    for filename in uploaded.keys():\n",
    "        df = pd.read_csv(f\"/content/Tiered_{filename}\")\n",
    "        sheet_name = filename.rsplit(\".\", 1)[0][:31]  # Excel sheet names max 31 chars\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"✅ Saved all tiered sheets into {output_excel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ez3pQkli3UeZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Ez3pQkli3UeZ",
    "outputId": "4acf6ad7-2063-4912-d1f7-543870c052ce"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_6b0dbc97-e72e-4c5d-8178-6b64952b0083\", \"Companies_Tiered_All.xlsx\", 7515)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files.download(output_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34f66c-157a-45b4-a39b-23b056dbb428",
   "metadata": {
    "id": "9b34f66c-157a-45b4-a39b-23b056dbb428"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
